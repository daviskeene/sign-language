{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Create a dictionary of numbers to letters so that we can decode the message at the end of training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z'}\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "\n",
    "letter_idx = 97\n",
    "for i in range(26):\n",
    "    word_dict[i] = chr(letter_idx)\n",
    "    letter_idx += 1\n",
    "\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Initialize the train and test batches with the ImageDataGenerator object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7202 images belonging to 26 classes.\n",
      "Found 624 images belonging to 26 classes.\n",
      "Batched train and test images...\n"
     ]
    }
   ],
   "source": [
    "train_path = './data/train'\n",
    "test_path = './data/test'\n",
    "\n",
    "# batch_size = How many do we want to predict? chooses images at random \n",
    "batch_size = 20\n",
    "class_mode = 'categorical'\n",
    "target_size = (64, 64)\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "train_batches = train_batches.flow_from_directory(directory=train_path, target_size=target_size, class_mode=class_mode, batch_size=batch_size, shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "test_batches = test_batches.flow_from_directory(directory=test_path, target_size=target_size, class_mode=class_mode, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Batched train and test images...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "#### 1) Create the model that correctly classifies each of the letters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1\n",
      "First convolution...\n",
      "Second convolution...\n",
      "Convolutions done\n",
      "Flattening...\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "kernel_size = (3, 3)\n",
    "strides = 2\n",
    "pool_size = (2, 2)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=kernel_size, activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=pool_size, strides=strides))\n",
    "\n",
    "print(\"First convolution...\")\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=kernel_size, activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=pool_size, strides=strides))\n",
    "\n",
    "print(\"Second convolution...\")\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=kernel_size, activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=pool_size, strides=strides))\n",
    "\n",
    "print(\"Convolutions done\")\n",
    "print(\"Flattening...\")\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "\n",
    "# Softmax must be equivalent to the number of classifications (26 for full alphabet)\n",
    "model.add(Dense(26,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) We compile and define the loss functions here so that we can create a fitted model in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling with ADAM model...\n",
      "Compiling with SGD model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling with ADAM model...\")\n",
    "lr = 0.001\n",
    "loss_f = 'categorical_crossentropy'\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=lr), loss=loss_f, metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "print(\"Compiling with SGD model...\")\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=lr), loss=loss_f, metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fit and save the model using the loss functions defined above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/9\n",
      "361/361 [==============================] - 23s 62ms/step - loss: 1.8243 - accuracy: 0.5333 - val_loss: 0.3172 - val_accuracy: 0.8846 - lr: 0.0010\n",
      "Epoch 2/9\n",
      "361/361 [==============================] - 22s 61ms/step - loss: 0.1858 - accuracy: 0.9461 - val_loss: 0.1213 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 3/9\n",
      "361/361 [==============================] - 23s 63ms/step - loss: 0.0585 - accuracy: 0.9854 - val_loss: 0.0539 - val_accuracy: 0.9776 - lr: 0.0010\n",
      "Epoch 4/9\n",
      "361/361 [==============================] - 25s 69ms/step - loss: 0.0188 - accuracy: 0.9964 - val_loss: 0.0374 - val_accuracy: 0.9872 - lr: 0.0010\n",
      "Epoch 5/9\n",
      "361/361 [==============================] - 21s 59ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 0.0847 - val_accuracy: 0.9696 - lr: 0.0010\n",
      "Epoch 6/9\n",
      "361/361 [==============================] - 21s 59ms/step - loss: 0.0047 - accuracy: 0.9999 - val_loss: 0.0352 - val_accuracy: 0.9872 - lr: 5.0000e-04\n",
      "Epoch 7/9\n",
      "361/361 [==============================] - 23s 65ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 0.9888 - lr: 5.0000e-04\n",
      "Epoch 8/9\n",
      "361/361 [==============================] - 24s 66ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9904 - lr: 5.0000e-04\n",
      "Epoch 9/9\n",
      "361/361 [==============================] - 24s 65ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9888 - lr: 5.0000e-04\n",
      "loss of 0.002300036372616887; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting model...\")\n",
    "epochs = 9\n",
    "fitted_model = model.fit(train_batches, epochs=epochs, callbacks=[reduce_lr, early_stop], validation_data=test_batches)\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Get new images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(test_batches)\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test our predictions using a batch size of: 20\n",
      "Accuracy: 0.0\n",
      "Actual labels: ['m', 'y', 'd', 'q', 'f', 'h', 'p', 'k', 'b', 'y', 'n', 'w', 'a', 'p', 'p', 'n', 'x', 'u', 'c', 'k']\n",
      "Predictions:   ['m', 'y', 'd', 'q', 'f', 'h', 'p', 'k', 'b', 'y', 'n', 'w', 'a', 'p', 'p', 'n', 'x', 'u', 'c', 'k']\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(imgs, verbose=0)\n",
    "\n",
    "print(\"Test our predictions using a batch size of:\", batch_size)\n",
    "\n",
    "# Iterate through predictions and the actual values and add to an array\n",
    "x = 0\n",
    "pred = []\n",
    "actual = []\n",
    "for ind, i in enumerate(predictions):\n",
    "    pred.append(word_dict[np.argmax(i)])\n",
    "    label = labels[x]\n",
    "    actual.append(word_dict[np.argmax(label)])\n",
    "    x += 1\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == pred[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(\"Accuracy:\", 1.0 * correct / total)\n",
    "print(\"Actual labels:\", actual)\n",
    "print(\"Predictions:  \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
